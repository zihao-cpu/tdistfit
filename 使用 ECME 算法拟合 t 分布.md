# 使用 ECME 算法拟合 t 分布

## 1. **函数概述**

`fitt` 函数实现了使用 **ECME 算法**（期望-条件最大化扩展算法）来拟合 t 分布的最大似然估计（MLE）。该算法由 Liu 和 Rubin 提出，并在 1995 年的文章中详细描述。其主要目标是通过迭代优化来估计 t 分布的三个参数：均值 ($$ \mu ​$$)、协方差矩阵 ($$ \Sigma​$$) 和 自由度参数 ($$ \nu ​$$)。

## 2. **背景和算法**

### 2.1 **t 分布的最大似然估计（MLE）**

我们通过 **最大似然估计（MLE）** 来估计 t 分布的参数。给定 $$ N$$ 个数据点 $$ x_1, x_2, \dots, x_N $$，最大似然估计的目标是最大化对数似然函数 $$ \mathcal{L}(\mu, \Sigma, \nu) $$，从而找到最佳的均值、协方差矩阵和自由度。

t 分布的对数似然函数（$$ \mathcal{L}$$）可以表示为：

$\mathcal{L}(\mu, \Sigma, \nu) = -\frac{N}{2} \log(\det(\Sigma)) - \frac{N\nu}{2} \log\left( 1 + \frac{1}{\nu} (x_i - \mu)^T \Sigma^{-1} (x_i - \mu) \right) + \text{常数项}$$

我们的目标是通过最大化 $$ \mathcal{L} $$ 来估计 $$ \mu $$、$$ \Sigma $$ 和 $$ \nu $$。

### 2.2 **ECME 算法简介**

ECME 是 EM（期望-最大化）算法的扩展，具体步骤为：
1. **E 步骤**：计算每个数据点的责任（即每个数据点对当前模型的贡献，通常以权重 $$ w_i $$ 表示）。
2. **CM 步骤**：在给定责任的情况下，最大化对数似然函数，更新模型参数（$$ \mu $$、$$ \Sigma $$、$$ \nu $$）。

ECME 算法交替执行这两个步骤，通过迭代收敛，最终获得 t 分布的最优参数。

## 3. **E 步骤：期望计算**

在 **E 步骤** 中，我们通过计算每个数据点的 **Mahalanobis 距离** 来计算数据点与当前模型的“匹配度”。这里，我们的目标是 **计算每个数据点的责任**，这与期望最大化（EM）算法的 **期望步骤**（E 步）是相同的。

### 3.1 **Mahalanobis 距离**

Mahalanobis 距离是一个衡量数据点与当前均值和协方差矩阵之间距离的标准化度量。它计算的是数据点 $$ x_i $$ 与均值 $$ \mu $$ 和协方差矩阵 $$ \Sigma $$ 的差异：

$$\delta_i = (x_i - \mu)^T \Sigma^{-1} (x_i - \mu)$$

这里，$$ \Sigma^{-1} $$ 是协方差矩阵的逆，$$ \delta_i $$ 是每个数据点的 Mahalanobis 距离。

### 3.2 **计算权重（责任）**

E 步骤的核心是 **计算每个数据点的权重**（责任）。这些权重 $$ w_i $$ 表示每个数据点 $$ x_i $$ 在当前模型下的 **贡献度**，基于其与当前均值和协方差的匹配度。权重的计算公式为：

$$w_i = \frac{\nu + p}{\delta_i + \nu}$$

其中：
- $$  p  $$ 是数据的维度（特征数）。
- $$  \nu  $$ 是自由度参数。
- $$  \delta_i  $$ 是每个数据点的 **Mahalanobis 距离**，衡量该点与当前模型的偏差。

该公式的意义在于：数据点与当前模型越接近（即 $$ \delta_i $$ 越小），其权重 $$ w_i $$ 越大，反之则越小。

### 3.3 **E 步骤总结**

E 步骤的主要任务是计算每个数据点对当前模型的 **期望贡献**，也就是根据当前的参数估计，计算每个数据点的权重。这个过程通过 Mahalanobis 距离来度量数据点与模型的偏差，进而计算每个数据点的责任。

## 4. **CM 步骤：条件最大化**

在 **CM 步骤** 中，我们利用 E 步骤中计算的权重 $$ w_i $$ 来更新 t 分布的参数：**均值** ($$ \mu $$)、**协方差矩阵** ($$ \Sigma $$) 和 **自由度** ($$ \nu $$)。

### 4.1 **对均值 $ \mu $ 的求导**

我们通过 **最大化对数似然函数** 来估计参数。首先，针对均值 $$ \mu $$，我们对对数似然函数 $$ \mathcal{L}(\mu, \Sigma, \nu) $$ 对 $$ \mu $$ 求导。对数似然函数中关于 $$ \mu $$ 的部分是：

$$\mathcal{L}(\mu) = -\frac{N}{2} \log(\det(\Sigma)) - \frac{N\nu}{2} \sum_{i=1}^{N} \log \left( 1 + \frac{1}{\nu} (x_i - \mu)^T \Sigma^{-1} (x_i - \mu) \right)$$

对 $$ \mu $$ 求导，得到：

$$\frac{\partial \mathcal{L}}{\partial \mu} = \frac{N}{2} \sum_{i=1}^{N} w_i \cdot \Sigma^{-1} (x_i - \mu)$$

其中 $$ w_i $$ 是由 **E 步骤** 计算得到的权重。为了使对数似然最大化，我们将导数设为零，得到更新的均值 $$ \mu $$：

$$\mu = \frac{\sum_{i=1}^{N} w_i x_i}{\sum_{i=1}^{N} w_i}$$

这个更新公式表明，新的均值是加权平均值，其中权重由每个数据点的 Mahalanobis 距离决定。

### 4.2 **对协方差矩阵 $ \Sigma $ 的求导**

协方差矩阵 $$ \Sigma $$ 的更新也是通过对对数似然函数的导数求解得到的。首先，我们提取对数似然函数中与协方差矩阵相关的部分：

$$\mathcal{L}(\Sigma) = -\frac{N}{2} \log(\det(\Sigma)) + \text{常数项}$$

对 $$ \Sigma $$ 求导，得到：

$$\frac{\partial \mathcal{L}}{\partial \Sigma} = \frac{N}{2} \left( \Sigma^{-1} - \Sigma^{-1} (x_i - \mu)(x_i - \mu)^T \Sigma^{-1} \right)$$

将其设置为零，得到协方差矩阵的更新公式：

$$\Sigma = \frac{1}{N} \sum_{i=1}^{N} w_i (x_i - \mu)(x_i - \mu)^T$$

这个公式表示，新的协方差矩阵是加权协方差，其中每个数据点的贡献权重由其 Mahalanobis 距离决定。

### 4.3 **自由度 $ \nu $ 的更新**

自由度参数 $$ \nu $$ 是通过 **非线性最小化** 来更新的。我们通过最小化负对数似然函数来优化自由度参数，具体的更新公式通过数值优化方法（如 `fsolve`）来求解。自由度更新的目标是通过最小化如下目标函数来实现：

$$\nu = \text{argmin} \, \mathcal{L}(\nu)$$

这个过程没有显式的解析解，通常通过数值方法（例如牛顿法或 fsolve）来找到最优解。

### 4.4 **CM 步骤总结**

CM 步骤通过最大化对数似然函数来更新参数 $$ \mu $$、$$ \Sigma $$ 和 $$ \nu $$。这个步骤利用 E 步骤中计算的权重 $$ w_i $$ 来更新参数。

## 5. **收敛标准**

在算法迭代过程中，我们通过 **熵**（entropy）作为收敛标准来判断算法是否收敛。熵 $$ H $$ 由以下公式给出：

$$H = \sum \log(\text{diag}(S)) + \log\left( \frac{(\nu \pi)^p}{\Gamma\left(\frac{\nu + p}{2}\right)} \right) - \Gamma\left(\frac{\nu + p}{2}\right)$$

其中：
- $$ S $$ 是协方差矩阵的对角元素。
- $$ \Gamma $$ 是 Gamma 函数。

当熵的变化小于预设的容忍度时，算法认为已经收敛。

## 6. **自由度 $ \nu $ 的优化**

自由度 $$ \nu $$ 通过 **非线性最小化**（如 `fsolve`）来更新，最小化负对数似然来求解最优自由度。这是通过数值优化来完成的，通常无法通过显式的解析解来获得。

## 7. **总结**

- **E 步骤** 通过计算每个数据点的 **Mahalanobis 距离** 和 **权重**（责任），为后续的最大化步骤提供数据点的贡献度。
- **CM 步骤** 通过 **最大化对数似然函数** 来更新参数 $$ \mu $$、$$ \Sigma $$ 和 $$ \nu $$。
- **自由度 $$ \nu $$** 的更新通过 **数值优化** 来实现，最小化负对数似然函数。
- **收敛标准** 通过 **熵** 来检测算法的收敛性。

通过交替执行 E 步骤和 CM 步骤，算法逐步收敛，最终获得 t 分布的最优参数 $$ \mu $$、$$ \Sigma $$ 和 $$ \nu $$。